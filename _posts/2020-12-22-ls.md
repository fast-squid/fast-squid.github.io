---
layout: post
title: "Least Squares"
background: '/img/posts/02.jpg'
category: slam
use_math: true

---

# Least Squares
<figure>
<img class="img-fluid" src="/img/posts/dram.jpg" alt="DRAM">
<figcaption>Fig 1. DRAM</figcaption>
</figure>
**Least Squares**... 이름부터 생소한 최소제곱법은 슬프게도 SLAM을 공부하면서 정말 필수적인 개념인듯 하더라구요. 수식이 많으면 많을수록 미루고싶은 마음이 솟구쳐서 미뤄왔었는데 모르면 안되겠다 싶어서 공부를 시작했습니다.

대체 이놈은 왜 필요한걸까요? 어떤 문제를 해결하고자 할때 값을 정확하게 측정할 수 없는 경우 실제 $x$를 직접 구할 수 없기 때문에 $x$에 가장 가까운 $\hat{x}$을 구하기 위해 Least Squares 를 사용합니다. 
Least Squares는 SLAM 과 같이 최적화가 필요한 많은 분야에서 사용된다고 하는군요. 하지만 우리는 SLAM, 그중에서도 Graph-Based SLAM에 적용하기 위해 알아야 합니다.

## Minizing Error Function
왜 모든 문제에서 값을 정확하게 측정할 수 없을까?
로봇이나 차량 내부에 완전 무결한 센서가 있다면 Wheel odometry를 통해 시작점만 알고 있다면 현재의 Pose를 구하는 Localization이 가능할 것입니다.
하지만 실제 센서들은 센서노이즈 등의 이유로 완벽하지가 않습니다.
간단하게 예시를 들어봅시다.
$\$ \hat{z} = f(x) $\$
$\$ \hat{z} + e = z $\$
 우리가 어떤 값 $x$를 카메라로 관찰하면 $f(x)$의 Transformation 을 거쳐 $\hat{z}$을 얻게 됩니다.
 실제 $z$는 에러 $e$를 보정해야 얻을 수 있겠죠.
 $\$ e(x) = z - f(x) $\$
 그리고 Cost Function을 *e*를 정의할 수 있습니다.
 $\$ e' = e^{T} \Omega e $\$
 
 Cost function이 왜 저렇게 나오는건지는 찾아봐야겠네요...
 
아무튼 우리는 Cost를 최소화하는 것이 목적입니다. SLAM에 적용하면 $e(x)$가 최소일때의 로봇의 Pose $x^{\*}$를 구하는것이죠.
$\$ x^{\*} = argmin\sum{e(x)} $\$

### argmin
#### 5-step process
1. Linearize(Taylor expansion) the error terms around the current solution / initial guess
1. Compute the first dervative of the squared error function
1. Set derivative to zero and solve linear system
1. Obtain the new state
1. Iterate

첫째로 Cost Function을 최소화하기위해 테일러급수를 구해줍니다. 테일러급수란 어떤 함수 f(x)를 무한급수로 표현하는 것을 뜻합니다.
$\$f(x) = \sum_{k=0}^{\infty}\cfrac{f^{(k)}(a)}{k!}(x-a)^k $\$
물론 컴퓨터로 계산하기 위해서는 앞의 도함수, 2계도함수까지만 전개해줍니다.
$\$ e(x+\Delta{x}) \approx e(x+\Delta{x})+J(x) $\$
$\$ e'(x+\Delta{x}) = e^{T}(x+\Delta{x}) \Omega e(x+\Delta{x}) $\$
$\$ e'(x+\Delta{x}) \approx (e+J\Delta{x})^{T}\Omega (e+J\Delta{x}) $\$

결과적으로 얻어진 식을 전개해서 풀면 다음과 같습니다.
$\$
e'(x+\Delta{x}) \approx e^{T}\Omega e + e^{T}\Omega J \Delta{x} + \Delta{x}^{T}J^{T}\Omega{e} + \Delta{x}^{T}J^{T}\Omega{J}\Delta{x}
$\$

첫째 항은 x에 대한 식이 아니므로 C(constant)라고 두고 2,3 항은 Linearly dependent, 마지막은 Qaudratic dependency를 갖고 있습니다.
함수의 최소 혹은 최대값은 Derivate가 0일때의 값이기 때문에 미분을 해줍니다.
Constant는 없어지고 일차항의 계수를 b, 2차항의 계수를 H라 두면 다음과 같이 표현됩니다.
$\$
\cfrac{\partial{F(x+\Delta{x}}}{\partial\Delta{x}} = 2b + 2H\Delta{x} = 0
$\$
$\$ H\Delta{x} = -b $\$

여기까지 왔으면 다 끝난것이나 마찬가지죠. $H$의 역행렬을 양변에 곱하면 되니까요.
하지만 H의 역행렬이 존재하지 않을수 있습니다.
- $H$ is not a square matrix
- $Det(H) = 0$
H가 정사각행렬이 아닌경우 정사각행렬을 만들어주기 위해 다음과 같은 작업을 해줍니다.
$\$ H^{T}H\Delta{x} = -H^{T}b $\$
$\$ \Delta{x} = -(H^{T}H)^{-1}Hb $\$

이런 모든 과정은 사실 계산량이 많아 Computationally efficient 하다고 볼 수 없습니다. 이런 문제들을 해결하기 위해
**Cholesky Factorization, Conjugate Gradient** 방식을 쓴다고 하네요.
하지만 여기선 다루진 않겠습니다! 저도 잘 모르니까요!


